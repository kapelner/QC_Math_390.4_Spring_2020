---
title: "Lab 6"
author: "Your Name Here"
output: pdf_document
date: "11:59PM March 18, 2020"
---

Load the Boston Housing data and create the vector `y`, design matrix `X` and let `n` and `p_plus_one` be the number of rows and columns.

```{r}
y = MASS::Boston$medv
X = as.matrix(cbind(1, MASS::Boston[, 1 : 13]))
n = nrow(X)
p_plus_one = ncol(X)
```

Create a new materix `Xjunk` by adding random columns to `X` to make the number of columns and rows the same.

```{r}
#TODO
```

Test that the projection matrix onto $colsp[Xjunk]$ is the same as $I_n$:

```{r}
pacman::p_load(testthat)
#TODO
```


Write a function spec'd as follows:

```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  #TODO
  list(a_parallel = a_parallel, a_perpendicular = a_perpendicular)
}
```

Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
#prediction:
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
#prediction:
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7))
t(result$a_parallel) %*% result$a_perpendicular
#prediction:
result$a_parallel + result$a_perpendicular
#prediction:
result$a_parallel / c(1, 3, 5 ,7)
#prediction:
```


Try to project onto the column space of $X$ by projecting $y$ on each vector of $X$ individually and adding up the projections. You can use the function `orthogonal_projection`.

```{r}
#TODO
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
yhat = lm(y ~ X)$fitted.values
#TODO
```

Convert $X$ into $V$ where $V$ has the same column space as $X$ but has orthogonal columns. You can use the function `orthogonal_projection`. This is essentially gram-schmidt.

```{r}
V = matrix(NA, nrow = nrow(X), ncol = ncol(X))
#TODO
```

Convert $V$ into $Q$ whose columns are the same except normalized

```{r}
Q = matrix(NA, nrow = nrow(X), ncol = ncol(X))
#TODO
```

Verify $Q^T$ is the inverse of $Q$ i.e. a unitary matrix.

```{r}
#TODO
```


Project $Y$ onto $colsp[Q]$ and verify it is the same as the OLS fit.

```{r}
#TODO
```


Project $Y$ onto the columns of $Q$ one by one and verify it sums to be the projection onto the whole space.

```{r}
#TODO
```

Verify the OLS fit squared length is the sum of squared lengths of each of the orthogonal projections.

```{r}
#TODO
```

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
#TODO
```
Find the $s_e$ in sample and out of sample. Which one is greater? Note: we are now using $s_e$ and not RMSE since RMSE has the $-(p + 1)$ in the denominator which makes comparison more difficult when the $n$'s are different.

```{r}
#TODO
```

Do these two exercises 1,000 times and find the average difference between $s_e$ and oos$s_e$. This is just `sd(e)` the standard deviation of the residuals.

```{r}
#TODO
```

Using `Xjunk` from above, divide the data into training and testing sets. Fit the model in-sample and calculate $s_e$ in-sample by varying the number of columns used beginning with the first column. Keep the $s_e$ values in the variable `s_es` which has length $n$. Show that it reaches 0 at $n$ i.e. the model overfits.

```{r}
#TODO
```

Do the same thing but now calculate oos$s_e$. Does this go to zero? What is the index corresponding to the best model?

```{r}
#TODO
```

Beginning with the Boston Housing Data matrix `X`, pull out the second column, the `crim` feature and call it `x2`. Then, use the `cut` function to bin each of its $n$ values into two bins: the first is all values <= the median of `crim` and the second is all values > median of `crim`. Call it `x2bin`. Use the `table` function to ensure that half of the values are in the first group and half in the second group. This requires reading the documentation for `cut` carefully and using the `quantile` function carefully.

```{r}
#TODO
```

Now convert the factor variable `x2bin` to two dummies, `X2dummy`, a matrix of $n \times 2$ and verify the rowsums are all 1. They must be 1 because either the value is <= median or > median.

```{r}
#TODO
```

Drop the first column of this matrix to arrive at `X2dummyfeatures`.

```{r}
#TODO
```


What you did with `crim`, do for all 13 variables in the Boston housing data, ie create `X2dummyfeatures` for all and then column bind them all together into a massive `Xdummy` matrix. Then run a regression of $y$ on those features and report $R^2$.


```{r}
#TODO
```

Create a new `Xdummy` matrix. This time with two dummies for each variable: (1) between the 33%ile and 66%ile and (2) greater than the 66%ile. Run the regression and report $R^2$.


```{r}
#TODO
```

Keep doing this until each variable has 31 dummies for a total of $p = 403 + 1$ variables. Report all $R^2$;s. Why is it increasing and why is the last one so high?

```{r}
#TODO
```

Repeat this exercise with a 20% test set held out. Record in sample $s_e$'s and oos$s_e$'s. Do we see the canonical picture?

```{r}
#TODO
```

What is the optimal number of bins for each feature? That is what is the optimal complexity model of this set of models?

```{r}
#TODO
```







